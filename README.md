# MLOps Project - Vehicle Insurance Data Pipeline

End-to-end production-style MLOps pipeline for vehicle insurance prediction, built to demonstrate real-world machine learning system design, pipeline orchestration, data reliability, and deployment readiness.

---

## Project Overview

This project focuses on the **importance of MLOps** rather than just model building.  
It demonstrates how machine learning workflows are structured, validated, versioned, and served in a production-like environment.

The system predicts whether a customer is likely to opt for vehicle insurance based on demographic and policy-related features.

---
## ðŸ“Œ Project Structure

```text
vehicle-insurance-mlops/
â”‚
â”œâ”€â”€ app.py                      # FastAPI entry point for prediction service
â”œâ”€â”€ Dockerfile                  # Docker configuration for deployment
â”œâ”€â”€ requirements.txt            # Project dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ pyproject.toml              # Build system configuration
â”œâ”€â”€ README.md                   # Project documentation
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ schema.yaml             # Dataset schema definition
â”‚   â””â”€â”€ model.yaml              # Model & training configuration
â”‚
â”œâ”€â”€ artifact/                   # Versioned pipeline outputs (timestamp-based)
â”‚   â””â”€â”€ <timestamp>/
â”‚       â”œâ”€â”€ data_ingestion/     # Raw & split datasets
â”‚       â”œâ”€â”€ data_validation/    # Validation reports
â”‚       â”œâ”€â”€ data_transformation/# Transformed data & preprocessors
â”‚       â””â”€â”€ model_trainer/      # Trained model artifacts
â”‚
â”œâ”€â”€ logs/                       # Centralized pipeline execution logs
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ data.csv                # Sample / reference dataset
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/             # Core ML pipeline components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â””â”€â”€ model_pusher.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/               # Training & prediction pipelines
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â”‚   â””â”€â”€ prediction_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/                 # Configuration & artifact data classes
â”‚   â”œâ”€â”€ configuration/          # MongoDB / AWS connection logic
â”‚   â”œâ”€â”€ cloud_storage/          # AWS S3 interaction layer
â”‚   â”œâ”€â”€ data_access/            # Data fetching layer
â”‚   â”œâ”€â”€ utils/                  # Common utility functions
â”‚   â”œâ”€â”€ logger/                 # Centralized logging module
â”‚   â””â”€â”€ exception/              # Custom exception handling
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ css/
â”‚       â””â”€â”€ style.css           # UI styling
â”‚
â””â”€â”€ templates/
    â””â”€â”€ vehicledata.html        # HTML template for prediction UI

```


## Why This Is an MLOps Project

This is not a notebook-only ML project.  
It showcases how real ML systems are built with:

- Modular pipeline architecture
- Data ingestion from external sources
- Schema-based data validation
- Feature engineering and preprocessing pipelines
- Model training with evaluation
- Versioned artifacts
- Centralized logging and exception handling
- API-based prediction service
- Dockerized local deployment

The emphasis is on **reproducibility, reliability, and maintainability**.

---

## End-to-End Workflow

MongoDB  
â†’ Data Ingestion  
â†’ Data Validation (Schema-based)  
â†’ Data Transformation & Feature Engineering  
â†’ Model Training & Evaluation  
â†’ Prediction Pipeline (FastAPI)  
â†’ Dockerized Local Deployment  

---

## Data Source

- MongoDB Atlas is used as the data source
- Data is fetched dynamically using environment variables
- No hardcoded credentials are used





---

## Pipeline Components

### Data Ingestion
- Fetches data from MongoDB
- Performs train-test split
- Stores datasets as versioned artifacts

- ![Data Ingestion](data_ingestion.png)

### Data Validation
- Schema validation using a predefined schema file
- Checks required columns and data types
- Generates validation reports as artifacts

- ![Data Validation](data_validation.png)

### Data Transformation
- Feature engineering
- Encoding categorical variables
- Scaling numerical features
- Handling class imbalance using SMOTEENN
- Saves preprocessing pipeline for reuse

- ![Data Transformation](data_transformation.png)

### Model Training
- Model trained using RandomForestClassifier
- Evaluated using precision, recall, and F1-score
- Best-performing model saved as a versioned artifact


![Model Training](model_training.png)


---

## Prediction Service

- Built using FastAPI
- HTML-based form for user input
- Accepts customer details
- Returns prediction result:
  - Response-Yes
  - Response-No
 
  - 

The prediction pipeline reuses the trained model and preprocessing object to ensure consistency.

---

<p align="center">
  <img src="banner1.png" alt="Local Deployment" width="400">
</p>



---

## Docker Usage (Local Only)

Docker is used **only for local containerized execution** to demonstrate deployment readiness.


