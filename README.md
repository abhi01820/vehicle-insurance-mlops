# MLOps Project - Vehicle Insurance Data Pipeline

[![CI/CD Pipeline](https://github.com/YOUR_USERNAME/vehicle-insurance-mlops/actions/workflows/ci-cd.yml/badge.svg)](https://github.com/YOUR_USERNAME/vehicle-insurance-mlops/actions/workflows/ci-cd.yml)
[![Docker Build](https://github.com/YOUR_USERNAME/vehicle-insurance-mlops/actions/workflows/docker.yml/badge.svg)](https://github.com/YOUR_USERNAME/vehicle-insurance-mlops/actions/workflows/docker.yml)
[![Python 3.10](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/release/python-3100/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

End-to-end production-style MLOps pipeline for vehicle insurance prediction, built to demonstrate real-world machine learning system design, pipeline orchestration, data reliability, and deployment readiness.

---

## Project Overview

This project focuses on the **importance of MLOps** rather than just model building.  
It demonstrates how machine learning workflows are structured, validated, versioned, and served in a production-like environment.

The system predicts whether a customer is likely to opt for vehicle insurance based on demographic and policy-related features.

---
## ðŸ“Œ Project Structure

```text
vehicle-insurance-mlops/
â”‚
â”œâ”€â”€ app.py                      # FastAPI entry point for prediction service
â”œâ”€â”€ Dockerfile                  # Docker configuration for deployment
â”œâ”€â”€ requirements.txt            # Project dependencies
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ pyproject.toml              # Build system configuration
â”œâ”€â”€ README.md                   # Project documentation
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ schema.yaml             # Dataset schema definition
â”‚   â””â”€â”€ model.yaml              # Model & training configuration
â”‚
â”œâ”€â”€ artifact/                   # Versioned pipeline outputs (timestamp-based)
â”‚   â””â”€â”€ <timestamp>/
â”‚       â”œâ”€â”€ data_ingestion/     # Raw & split datasets
â”‚       â”œâ”€â”€ data_validation/    # Validation reports
â”‚       â”œâ”€â”€ data_transformation/# Transformed data & preprocessors
â”‚       â””â”€â”€ model_trainer/      # Trained model artifacts
â”‚
â”œâ”€â”€ logs/                       # Centralized pipeline execution logs
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ data.csv                # Sample / reference dataset
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/             # Core ML pipeline components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â””â”€â”€ model_pusher.py
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/               # Training & prediction pipelines
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py
â”‚   â”‚   â””â”€â”€ prediction_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/                 # Configuration & artifact data classes
â”‚   â”œâ”€â”€ configuration/          # MongoDB / AWS connection logic
â”‚   â”œâ”€â”€ cloud_storage/          # AWS S3 interaction layer
â”‚   â”œâ”€â”€ data_access/            # Data fetching layer
â”‚   â”œâ”€â”€ utils/                  # Common utility functions
â”‚   â”œâ”€â”€ logger/                 # Centralized logging module
â”‚   â””â”€â”€ exception/              # Custom exception handling
â”‚
â”œâ”€â”€ static/
â”‚   â””â”€â”€ css/
â”‚       â””â”€â”€ style.css           # UI styling
â”‚
â””â”€â”€ templates/
    â””â”€â”€ vehicledata.html        # HTML template for prediction UI

```


## Why This Is an MLOps Project

This is not a notebook-only ML project.  
It showcases how real ML systems are built with:

- Modular pipeline architecture
- Data ingestion from external sources
- Schema-based data validation
- Feature engineering and preprocessing pipelines
- Model training with evaluation
- Versioned artifacts
- Centralized logging and exception handling
- API-based prediction service
- Dockerized local deployment

The emphasis is on **reproducibility, reliability, and maintainability**.

---

## End-to-End Workflow

MongoDB  
â†’ Data Ingestion  
â†’ Data Validation (Schema-based)  
â†’ Data Transformation & Feature Engineering  
â†’ Model Training & Evaluation  
â†’ Prediction Pipeline (FastAPI)  
â†’ Dockerized Local Deployment  

---

## Data Source

- MongoDB Atlas is used as the data source
- Data is fetched dynamically using environment variables
- No hardcoded credentials are used





---

## Pipeline Components

### Data Ingestion
- Fetches data from MongoDB
- Performs train-test split
- Stores datasets as versioned artifacts

- ![Data Ingestion](data_ingestion.png)

### Data Validation
- Schema validation using a predefined schema file
- Checks required columns and data types
- Generates validation reports as artifacts

- ![Data Validation](data_validation.png)

### Data Transformation
- Feature engineering
- Encoding categorical variables
- Scaling numerical features
- Handling class imbalance using SMOTEENN
- Saves preprocessing pipeline for reuse

- ![Data Transformation](data_transformation.png)

### Model Training
- Model trained using RandomForestClassifier
- Evaluated using precision, recall, and F1-score
- Best-performing model saved as a versioned artifact


![Model Training](model_training.png)


---

## Prediction Service

- Built using FastAPI
- HTML-based form for user input
- Accepts customer details
- Returns prediction result:
  - Response-Yes
  - Response-No
 
  - 

The prediction pipeline reuses the trained model and preprocessing object to ensure consistency.

---

<p align="center">
  <img src="banner1.png" alt="Local Deployment" width="400">
</p>



---

## ðŸš€ CI/CD Pipeline

This project includes a complete CI/CD pipeline using **GitHub Actions**:

### Automated Workflows

1. **CI/CD Pipeline** (`.github/workflows/ci-cd.yml`)
   - Runs on every push to main/master branch
   - **Testing**: Linting with flake8, unit tests with pytest
   - **Build**: Docker image creation with caching
   - **Push**: Automated push to Docker Hub (if configured)
   - **Deploy**: Deployment hooks (configure for your platform)

2. **Docker Build & Test** (`.github/workflows/docker.yml`)
   - Validates Dockerfile changes
   - Tests container health endpoint
   - Security scanning with Trivy

### Setup CI/CD

1. **GitHub Secrets** (Required for Docker Hub push):
   ```
   DOCKER_USERNAME: your-dockerhub-username
   DOCKER_PASSWORD: your-dockerhub-token
   ```

2. **Pre-commit Hooks** (Optional but recommended):
   ```bash
   pip install pre-commit
   pre-commit install
   ```

3. **Run Tests Locally**:
   ```bash
   pytest tests/ --cov=src --cov-report=term
   ```

---

## Docker Usage

### Using Docker Compose (Recommended)
```bash
docker-compose up -d
```

Access at http://localhost:5000

Stop:
```bash
docker-compose down
```

### Manual Docker Build & Run
```bash
docker build -t vehicle-insurance-mlops:latest .
docker run -p 5000:5000 vehicle-insurance-mlops:latest
```

### With Volume Mounts (for artifacts)
```bash
docker run -p 5000:5000 \
  -v "$(pwd)/artifact:/app/artifact" \
  -v "$(pwd)/logs:/app/logs" \
  --env-file .env \
  vehicle-insurance-mlops:latest
```


